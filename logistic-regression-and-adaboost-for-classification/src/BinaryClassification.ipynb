{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and AdaBoost for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NumPy Documentation:** https://numpy.org/doc/  \n",
    "**`pandas` Documentation:** https://pandas.pydata.org/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do\n",
    "np.random.seed(1)\n",
    "\n",
    "# ref: https://stackoverflow.com/questions/14861891/runtimewarning-invalid-value-encountered-in-divide/54364060\n",
    "# ref: https://numpy.org/doc/stable/reference/generated/numpy.seterr.html\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "# ref: https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sklearn.impute.SimpleImputer`:** https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html  \n",
    "**`sklearn.preprocessing.StandardScaler`:** https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html  \n",
    "**`sklearn.model_selection.train_test_split`:** https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Preprocessing  \n",
    "\n",
    "### Reference:  \n",
    "- **Preprocessing data:** https://scikit-learn.org/stable/modules/preprocessing.html  \n",
    "- **Imputation of missing values:** https://scikit-learn.org/stable/modules/impute.html  \n",
    "- **Easy Guide to Data Preprocessing in Python:** https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html  \n",
    "- **Data preprocessing:** https://cs.ccsu.edu/~markov/ccsu_courses/DataMining-3.html  \n",
    "\n",
    "### Datasets:  \n",
    "- **Telco Customer Churn:** https://www.kaggle.com/blastchar/telco-customer-churn  \n",
    "- **Adult Salary Scale:** https://archive.ics.uci.edu/ml/datasets/adult  \n",
    "- **Credit Card Fraud Detection:** https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telco Customer Churn Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telco_customer_churn_dataset_preprocessing():\n",
    "    data_frame = pd.read_csv('./datasets/telco-customer-churn.csv')\n",
    "    \n",
    "    # imputing missing values in specific columns\n",
    "    numeric_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    \n",
    "    data_frame['TotalCharges'].replace({' ': np.nan}, inplace=True)\n",
    "    data_frame['TotalCharges'] = numeric_imputer.fit_transform(data_frame[['TotalCharges']])\n",
    "    \n",
    "    # dropping unnecessary columns\n",
    "    data_frame.drop(['customerID'], inplace=True, axis=1)\n",
    "    \n",
    "    # modifying values in specific columns\n",
    "    data_frame['MultipleLines'].replace({'No phone service': 'No'}, inplace=True)\n",
    "    \n",
    "    for key in ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']:\n",
    "        data_frame[key].replace({'No internet service': 'No'}, inplace=True)\n",
    "    \n",
    "    data_frame['Churn'].replace({'Yes': 1, 'No': 0}, inplace=True)\n",
    "    \n",
    "    # encoding categorical features\n",
    "    data_frame = pd.get_dummies(\n",
    "        data_frame, \n",
    "        columns=[\n",
    "            'gender', 'SeniorCitizen', 'Partner', 'Dependents', \n",
    "            'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', \n",
    "            'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', \n",
    "            'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # separating target column from data_frame\n",
    "    data_frame_target = data_frame['Churn']\n",
    "    data_frame.drop(['Churn'], inplace=True, axis=1)\n",
    "    \n",
    "    # standardizing specific columns in data_frame\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    for key in ['tenure', 'MonthlyCharges', 'TotalCharges']:\n",
    "        data_frame[key] = standard_scaler.fit_transform(data_frame[[key]])\n",
    "    \n",
    "    return data_frame.to_numpy(), data_frame_target.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Salary Scale Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adult_dataset_preprocessing():\n",
    "    train_data_frame = pd.read_csv('./datasets/adult-train.csv')\n",
    "    test_data_frame = pd.read_csv('./datasets/adult-test.csv')\n",
    "    \n",
    "    # modifying values in specific columns\n",
    "    train_data_frame['salary-scale'].replace({' >50K': 1, ' <=50K': 0}, inplace=True)\n",
    "    test_data_frame['salary-scale'].replace({' >50K.': 1, ' <=50K.': 0}, inplace=True)\n",
    "    \n",
    "    # concatenating train_data_frame and test_data_frame\n",
    "    # ref: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion/60425\n",
    "    train_data_frame['is_train'] = 1\n",
    "    test_data_frame['is_train'] = 0\n",
    "    data_frame = pd.concat([train_data_frame, test_data_frame], ignore_index=True)\n",
    "    \n",
    "    # imputing missing values in specific columns\n",
    "    categorical_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    \n",
    "    for key in ['workclass', 'occupation', 'native-country']:\n",
    "        data_frame[key].replace({' ?': np.nan}, inplace=True)\n",
    "        data_frame[key] = categorical_imputer.fit_transform(data_frame[[key]])\n",
    "    \n",
    "    # encoding categorical features\n",
    "    data_frame = pd.get_dummies(\n",
    "        data_frame, \n",
    "        columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "    )\n",
    "    data_frame.drop(['native-country_ Holand-Netherlands'], inplace=True, axis=1)\n",
    "    \n",
    "    # separating train_data_frame and test_data_frame from data_frame\n",
    "    train_data_frame = data_frame[data_frame['is_train'] == 1]\n",
    "    test_data_frame = data_frame[data_frame['is_train'] == 0]\n",
    "    \n",
    "    train_data_frame.drop(['is_train'], inplace=True, axis=1)\n",
    "    test_data_frame.drop(['is_train'], inplace=True, axis=1)\n",
    "    \n",
    "    # separating target column from data_frames\n",
    "    train_data_frame_target = train_data_frame['salary-scale']\n",
    "    train_data_frame.drop(['salary-scale'], inplace=True, axis=1)\n",
    "    \n",
    "    test_data_frame_target = test_data_frame['salary-scale']\n",
    "    test_data_frame.drop(['salary-scale'], inplace=True, axis=1)\n",
    "    \n",
    "    # standardizing specific columns in data_frame\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    for key in ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']:\n",
    "        train_data_frame[key] = standard_scaler.fit_transform(train_data_frame[[key]])\n",
    "        test_data_frame[key] = standard_scaler.fit_transform(test_data_frame[[key]])   \n",
    "    \n",
    "    return (\n",
    "        train_data_frame.to_numpy(), \n",
    "        train_data_frame_target.to_numpy(), \n",
    "        test_data_frame.to_numpy(), \n",
    "        test_data_frame_target.to_numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Card Fraud Detection Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_fraud_detection_dataset_preprocessing(use_smaller_subset=False):\n",
    "    data_frame = pd.read_csv('./datasets/credit-card-fraud-detection.csv')\n",
    "    \n",
    "    # separating data samples based on value in 'Class' column\n",
    "    data_frame_0 = data_frame[data_frame['Class'] == 0]\n",
    "    data_frame_1 = data_frame[data_frame['Class'] == 1]\n",
    "    \n",
    "    # splitting data_frame_0 and data_frame_1 into training and testing sets\n",
    "    train_data_frame_0 = data_frame_0.sample(frac=0.8, random_state=1)\n",
    "    test_data_frame_0 = data_frame_0.drop(train_data_frame_0.index)\n",
    "    \n",
    "    if use_smaller_subset:\n",
    "        train_data_frame_0 = train_data_frame_0.sample(n=16000, random_state=1)\n",
    "        test_data_frame_0 = test_data_frame_0.sample(n=4000, random_state=1)\n",
    "    \n",
    "    train_data_frame_1 = data_frame_1.sample(frac=0.8, random_state=1)\n",
    "    test_data_frame_1 = data_frame_1.drop(train_data_frame_1.index)\n",
    "    \n",
    "    # concatenating train_data_frame_0 and train_data_frame_1, test_data_frame_0 and test_data_frame_1\n",
    "    train_data_frame = pd.concat([train_data_frame_0, train_data_frame_1], ignore_index=True).sample(frac=1, random_state=1)\n",
    "    test_data_frame = pd.concat([test_data_frame_0, test_data_frame_1], ignore_index=True).sample(frac=1, random_state=1)\n",
    "    \n",
    "    # separating target columns from data_frames\n",
    "    train_data_frame_target = train_data_frame['Class']\n",
    "    train_data_frame.drop(['Class'], inplace=True, axis=1)\n",
    "    \n",
    "    test_data_frame_target = test_data_frame['Class']\n",
    "    test_data_frame.drop(['Class'], inplace=True, axis=1)\n",
    "    \n",
    "    # standardizing specific columns in data_frames\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    for key in list(train_data_frame.columns):\n",
    "        train_data_frame[key] = standard_scaler.fit_transform(train_data_frame[[key]])\n",
    "        \n",
    "    for key in list(test_data_frame.columns):\n",
    "        test_data_frame[key] = standard_scaler.fit_transform(test_data_frame[[key]])\n",
    "    \n",
    "    return (\n",
    "        train_data_frame.to_numpy(), \n",
    "        train_data_frame_target.to_numpy(), \n",
    "        test_data_frame.to_numpy(), \n",
    "        test_data_frame_target.to_numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation  \n",
    "\n",
    "**Reference:** https://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_predicted):\n",
    "    return np.mean((y_true - y_predicted) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, y_true, y_predicted, is_weak_learning=False):\n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    # calculating gradients of loss with respect to weights w\n",
    "    if is_weak_learning:\n",
    "        # using tanh as logistic function\n",
    "        dw = np.dot(X.T, (y_true - y_predicted) * (1 - y_predicted ** 2)) / num_samples\n",
    "    else:\n",
    "        # using sigmoid as logistic function\n",
    "        dw = np.dot(X.T, (y_true - y_predicted) * y_predicted * (1 - y_predicted)) / num_samples\n",
    "    \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    # standardizing to have zero mean and unit variance\n",
    "    return (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y_true, epochs, learning_rate, report_loss=False, is_weak_learning=False, early_stopping_threshold=0):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # initializing weights w to zero\n",
    "    w = np.zeros((num_features + 1, 1))\n",
    "    \n",
    "    # normalizing inputs X\n",
    "    X = normalize(X)\n",
    "    \n",
    "    # augmenting dummy input attribute 1 to each row of X\n",
    "    X = np.concatenate((X, np.ones((num_samples, 1))), axis=1)\n",
    "    \n",
    "    # reshaping target y_true\n",
    "    y_true = y_true.reshape(num_samples, 1)\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        # calculating hypotheses\n",
    "        if is_weak_learning:\n",
    "            y_predicted = (1 + tanh(np.dot(X, w))) / 2\n",
    "        else:\n",
    "            y_predicted = sigmoid(np.dot(X, w))\n",
    "        \n",
    "        # calculating gradients of loss with respect to weights w\n",
    "        dw = gradients(X, y_true, y_predicted, is_weak_learning=is_weak_learning)\n",
    "        \n",
    "        # gradient descent: updating parameters weights w\n",
    "        w = w + learning_rate * dw\n",
    "        \n",
    "        # calculating MSE loss in this epoch\n",
    "        if is_weak_learning:\n",
    "            loss = mean_squared_error(y_true, (1 + tanh(np.dot(X, w))) / 2)\n",
    "        else:\n",
    "            loss = mean_squared_error(y_true, sigmoid(np.dot(X, w)))\n",
    "        \n",
    "        # reporting MSE loss in this epoch\n",
    "        if report_loss:\n",
    "            print(f'Epoch {epoch + 1}: MSE Loss = {loss}')\n",
    "        \n",
    "        # early termination of gradient descent\n",
    "        if loss <= early_stopping_threshold:\n",
    "            break\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, is_weak_learning=False):\n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    # normalizing inputs X\n",
    "    X = normalize(X)\n",
    "    \n",
    "    # augmenting dummy input attribute 1 to each row of X\n",
    "    X = np.concatenate((X, np.ones((num_samples, 1))), axis=1)\n",
    "    \n",
    "    # calculating hypotheses\n",
    "    if is_weak_learning:\n",
    "        y_predicted = (1 + tanh(np.dot(X, w))) / 2\n",
    "    else:\n",
    "        y_predicted = sigmoid(np.dot(X, w))\n",
    "    \n",
    "    # determining and storing predictions\n",
    "    predictions = [1 if y_pred >= 0.5 else 0 for y_pred in y_predicted]\n",
    "    \n",
    "    return np.array(predictions).reshape(num_samples, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_boosting(X, y_true, num_boosting_rounds, report_accuracy=False):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # initializing local variables\n",
    "    example_weights = np.full((num_samples), 1 / num_samples)\n",
    "    hypotheses = []\n",
    "    hypothesis_weights = []\n",
    "    \n",
    "    # boosting loop\n",
    "    for k in range(num_boosting_rounds):\n",
    "        # resampling input examples\n",
    "        examples = np.concatenate((X, y_true), axis=1)\n",
    "        \n",
    "        # ref: https://stackoverflow.com/questions/14262654/numpy-get-random-set-of-rows-from-2d-array\n",
    "        # ref: https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "        data = examples[np.random.choice(num_samples, size=num_samples, replace=True, p=example_weights)]\n",
    "        \n",
    "        data_X = data[:, :num_features]\n",
    "        data_y_true = data[:, -1:]\n",
    "        \n",
    "        # getting hypothesis from a learning algorithm\n",
    "        w = train(\n",
    "            data_X, \n",
    "            data_y_true, \n",
    "            epochs=1000, \n",
    "            learning_rate=0.01, \n",
    "            report_loss=False, \n",
    "            is_weak_learning=True, \n",
    "            early_stopping_threshold=0.2\n",
    "        )\n",
    "        \n",
    "        # predicting target values with hypothesis\n",
    "        y_predicted = predict(X, w, is_weak_learning=True)\n",
    "        \n",
    "        # reporting accuracy of hypothesis\n",
    "        if report_accuracy:\n",
    "            print(np.sum(y_true == y_predicted) / num_samples)\n",
    "        \n",
    "        # calculating error for hypothesis\n",
    "        error = 0\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            error = error + (example_weights[i] if y_true[i] != y_predicted[i] else 0)\n",
    "        \n",
    "        if error > 0.5:\n",
    "            continue\n",
    "        else:\n",
    "            hypotheses.append(w)\n",
    "        \n",
    "        # updating example_weights\n",
    "        for i in range(num_samples):\n",
    "            example_weights[i] = example_weights[i] * (error / (1 - error) if y_true[i] == y_predicted[i] else 1)\n",
    "        \n",
    "        example_weights = example_weights / example_weights.sum()\n",
    "        \n",
    "        # updating hypothesis_weights\n",
    "        hypothesis_weights.append(np.log2((1 - error) / error))\n",
    "    \n",
    "    return hypotheses, np.array(hypothesis_weights).reshape(len(hypotheses), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_majority_predict(X, hypotheses, hypothesis_weights):\n",
    "    num_samples = X.shape[0]\n",
    "    num_hypotheses = len(hypotheses)\n",
    "    \n",
    "    # normalizing inputs X\n",
    "    X = normalize(X)\n",
    "    \n",
    "    # augmenting dummy input attribute 1 to each row of X\n",
    "    X = np.concatenate((X, np.ones((num_samples, 1))), axis=1)\n",
    "    \n",
    "    # calculating hypotheses\n",
    "    y_predicteds = []\n",
    "    \n",
    "    for i in range(num_hypotheses):\n",
    "        y_predicted = (1 + tanh(np.dot(X, hypotheses[i]))) / 2\n",
    "        y_predicteds.append([1 if y_pred >= 0.5 else -1 for y_pred in y_predicted])\n",
    "        \n",
    "    y_predicteds = np.array(y_predicteds)\n",
    "    \n",
    "    # calculating weighted majority hypothesis and storing predictions\n",
    "    weighted_majority_hypothesis = np.dot(y_predicteds.T, hypothesis_weights)\n",
    "    predictions = [1 if y_pred >= 0 else 0 for y_pred in weighted_majority_hypothesis]\n",
    "    \n",
    "    return np.array(predictions).reshape(num_samples, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation  \n",
    "\n",
    "**Reference:** https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_evaluation(y_true, y_predicted):\n",
    "    num_samples = y_true.shape[0]\n",
    "    \n",
    "    # initializing confusion matrix outcomes\n",
    "    true_positive = 0\n",
    "    false_negative = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    \n",
    "    # calculating and storing confusion matrix outcomes\n",
    "    for i in range(num_samples):\n",
    "        if y_true[i] == 1:\n",
    "            if y_true[i] == y_predicted[i]:\n",
    "                true_positive = true_positive + 1\n",
    "            else:\n",
    "                false_negative = false_negative + 1\n",
    "        elif y_true[i] == 0:\n",
    "            if y_true[i] == y_predicted[i]:\n",
    "                true_negative = true_negative + 1\n",
    "            else:\n",
    "                false_positive = false_positive + 1\n",
    "    \n",
    "    # calculating and storing performance measures\n",
    "    accuracy = (true_positive + true_negative) / (true_positive + false_negative + true_negative + false_positive)\n",
    "    sensitivity = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    false_discovery_rate = false_positive / (true_positive + false_positive)\n",
    "    f1_score = 2 * sensitivity * precision / (sensitivity + precision)\n",
    "    \n",
    "    return (accuracy, sensitivity, specificity, precision, false_discovery_rate, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Dataset Features & Targets and Splitting Datasets into Training & Testing Sets  \n",
    "\n",
    "**Reference:** https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telco Customer Churn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_customer_churn_features, telco_customer_churn_target = telco_customer_churn_dataset_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_churn_features, \n",
    "    test_churn_features, \n",
    "    train_churn_target, \n",
    "    test_churn_target\n",
    ") = train_test_split(telco_customer_churn_features, telco_customer_churn_target, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_churn_target = train_churn_target.reshape(train_churn_target.shape[0], 1)\n",
    "test_churn_target = test_churn_target.reshape(test_churn_target.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Salary Scale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_salary_features, train_salary_target, test_salary_features, test_salary_target) = adult_dataset_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_salary_target = train_salary_target.reshape(train_salary_target.shape[0], 1)\n",
    "test_salary_target = test_salary_target.reshape(test_salary_target.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Card Fraud Detection Dataset (Entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_fraud_features, \n",
    "    train_fraud_target, \n",
    "    test_fraud_features, \n",
    "    test_fraud_target\n",
    ") = credit_card_fraud_detection_dataset_preprocessing(use_smaller_subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraud_target = train_fraud_target.reshape(train_fraud_target.shape[0], 1)\n",
    "test_fraud_target = test_fraud_target.reshape(test_fraud_target.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Card Fraud Detection Dataset (Subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_fraud_sub_features, \n",
    "    train_fraud_sub_target, \n",
    "    test_fraud_sub_features, \n",
    "    test_fraud_sub_target\n",
    ") = credit_card_fraud_detection_dataset_preprocessing(use_smaller_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraud_sub_target = train_fraud_sub_target.reshape(train_fraud_sub_target.shape[0], 1)\n",
    "test_fraud_sub_target = test_fraud_sub_target.reshape(test_fraud_sub_target.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with `sigmoid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Telco Customer Churn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sigmoid for Telco Customer Churn Dataset: Train\n",
      "Accuracy: 0.7781327653532126\n",
      "Sensitivity: 0.7061143984220908\n",
      "Specificity: 0.8047653780695356\n",
      "Precision: 0.5721896643580181\n",
      "False Discovery Rate: 0.4278103356419819\n",
      "F1 Score: 0.6321365509123013\n",
      "\n",
      "Logistic Regression with sigmoid for Telco Customer Churn Dataset: Test\n",
      "Accuracy: 0.7778566359119943\n",
      "Sensitivity: 0.7557471264367817\n",
      "Specificity: 0.7851083883129123\n",
      "Precision: 0.5356415478615071\n",
      "False Discovery Rate: 0.46435845213849286\n",
      "F1 Score: 0.6269368295589988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = train(\n",
    "    train_churn_features, \n",
    "    train_churn_target, \n",
    "    epochs=1000, \n",
    "    learning_rate=0.01, \n",
    "    report_loss=False, \n",
    "    is_weak_learning=False, \n",
    "    early_stopping_threshold=0\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(train_churn_target, predict(train_churn_features, w, is_weak_learning=False))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with sigmoid for Telco Customer Churn Dataset: Train\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(test_churn_target, predict(test_churn_features, w, is_weak_learning=False))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with sigmoid for Telco Customer Churn Dataset: Test\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adult Salary Scale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sigmoid for Adult Salary Scale Dataset: Train\n",
      "Accuracy: 0.8200608089432143\n",
      "Sensitivity: 0.7448029588062747\n",
      "Specificity: 0.8439320388349515\n",
      "Precision: 0.6021860177356156\n",
      "False Discovery Rate: 0.3978139822643844\n",
      "F1 Score: 0.6659444666172529\n",
      "\n",
      "Logistic Regression with sigmoid for Adult Salary Scale Dataset: Test\n",
      "Accuracy: 0.8195442540384498\n",
      "Sensitivity: 0.7433697347893916\n",
      "Specificity: 0.8431041415359871\n",
      "Precision: 0.5943866943866943\n",
      "False Discovery Rate: 0.4056133056133056\n",
      "F1 Score: 0.6605822550831792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = train(\n",
    "    train_salary_features, \n",
    "    train_salary_target, \n",
    "    epochs=1000, \n",
    "    learning_rate=0.01, \n",
    "    report_loss=False, \n",
    "    is_weak_learning=False, \n",
    "    early_stopping_threshold=0\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(train_salary_target, predict(train_salary_features, w, is_weak_learning=False))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with sigmoid for Adult Salary Scale Dataset: Train\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(test_salary_target, predict(test_salary_features, w, is_weak_learning=False))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with sigmoid for Adult Salary Scale Dataset: Test\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit Card Fraud Detection Dataset (Entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sigmoid for Credit Card Fraud Detection Dataset (Entire): Train\n",
      "Accuracy: 0.998854489435847\n",
      "Sensitivity: 0.42385786802030456\n",
      "Specificity: 0.9998505179114714\n",
      "Precision: 0.8308457711442786\n",
      "False Discovery Rate: 0.1691542288557214\n",
      "F1 Score: 0.561344537815126\n",
      "\n",
      "Logistic Regression with sigmoid for Credit Card Fraud Detection Dataset (Entire): Test\n",
      "Accuracy: 0.9989115359632029\n",
      "Sensitivity: 0.42857142857142855\n",
      "Specificity: 0.9998944832316269\n",
      "Precision: 0.875\n",
      "False Discovery Rate: 0.125\n",
      "F1 Score: 0.5753424657534246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = train(\n",
    "    train_fraud_features, \n",
    "    train_fraud_target, \n",
    "    epochs=1000, \n",
    "    learning_rate=0.01, \n",
    "    report_loss=False, \n",
    "    is_weak_learning=False, \n",
    "    early_stopping_threshold=0\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(train_fraud_target, predict(train_fraud_features, w, is_weak_learning=False))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with sigmoid for Credit Card Fraud Detection Dataset (Entire): Train\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(test_fraud_target, predict(test_fraud_features, w, is_weak_learning=False))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with sigmoid for Credit Card Fraud Detection Dataset (Entire): Test\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit Card Fraud Detection Dataset (Subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sigmoid for Credit Card Fraud Detection Dataset (Subsampled): Train\n",
      "Accuracy: 0.9951201659143589\n",
      "Sensitivity: 0.8045685279187818\n",
      "Specificity: 0.9998125\n",
      "Precision: 0.990625\n",
      "False Discovery Rate: 0.009375\n",
      "F1 Score: 0.8879551820728291\n",
      "\n",
      "Logistic Regression with sigmoid for Credit Card Fraud Detection Dataset (Subsampled): Test\n",
      "Accuracy: 0.9943875061005368\n",
      "Sensitivity: 0.7857142857142857\n",
      "Specificity: 0.9995\n",
      "Precision: 0.9746835443037974\n",
      "False Discovery Rate: 0.02531645569620253\n",
      "F1 Score: 0.8700564971751412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = train(\n",
    "    train_fraud_sub_features, \n",
    "    train_fraud_sub_target, \n",
    "    epochs=1000, \n",
    "    learning_rate=0.01, \n",
    "    report_loss=False, \n",
    "    is_weak_learning=False, \n",
    "    early_stopping_threshold=0\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(train_fraud_sub_target, predict(train_fraud_sub_features, w, is_weak_learning=False))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with sigmoid for Credit Card Fraud Detection Dataset (Subsampled): Train\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(test_fraud_sub_target, predict(test_fraud_sub_features, w, is_weak_learning=False))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with sigmoid for Credit Card Fraud Detection Dataset (Subsampled): Test\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with `tanh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Telco Customer Churn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with tanh for Telco Customer Churn Dataset: Train\n",
      "Accuracy: 0.8015619453319134\n",
      "Sensitivity: 0.5575279421433268\n",
      "Specificity: 0.8918064672988086\n",
      "Precision: 0.6558391337973705\n",
      "False Discovery Rate: 0.34416086620262953\n",
      "F1 Score: 0.6027007818052595\n",
      "\n",
      "Logistic Regression with tanh for Telco Customer Churn Dataset: Test\n",
      "Accuracy: 0.801277501774308\n",
      "Sensitivity: 0.5890804597701149\n",
      "Specificity: 0.8708765315739868\n",
      "Precision: 0.5994152046783626\n",
      "False Discovery Rate: 0.40058479532163743\n",
      "F1 Score: 0.5942028985507246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = train(\n",
    "    train_churn_features, \n",
    "    train_churn_target, \n",
    "    epochs=1000, \n",
    "    learning_rate=0.01, \n",
    "    report_loss=False, \n",
    "    is_weak_learning=True, \n",
    "    early_stopping_threshold=0\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(train_churn_target, predict(train_churn_features, w, is_weak_learning=True))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with tanh for Telco Customer Churn Dataset: Train\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(test_churn_target, predict(test_churn_features, w, is_weak_learning=True))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with tanh for Telco Customer Churn Dataset: Test\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adult Salary Scale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with tanh for Adult Salary Scale Dataset: Train\n",
      "Accuracy: 0.8441694051165505\n",
      "Sensitivity: 0.5883178166050249\n",
      "Specificity: 0.9253236245954692\n",
      "Precision: 0.7141972441554421\n",
      "False Discovery Rate: 0.285802755844558\n",
      "F1 Score: 0.6451748251748252\n",
      "\n",
      "Logistic Regression with tanh for Adult Salary Scale Dataset: Test\n",
      "Accuracy: 0.8450340888151834\n",
      "Sensitivity: 0.5852834113364535\n",
      "Specificity: 0.925371934057097\n",
      "Precision: 0.7080843032400126\n",
      "False Discovery Rate: 0.2919156967599874\n",
      "F1 Score: 0.6408540925266905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = train(\n",
    "    train_salary_features, \n",
    "    train_salary_target, \n",
    "    epochs=1000, \n",
    "    learning_rate=0.01, \n",
    "    report_loss=False, \n",
    "    is_weak_learning=True, \n",
    "    early_stopping_threshold=0\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(train_salary_target, predict(train_salary_features, w, is_weak_learning=True))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with tanh for Adult Salary Scale Dataset: Train\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(test_salary_target, predict(test_salary_features, w, is_weak_learning=True))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with tanh for Adult Salary Scale Dataset: Test\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit Card Fraud Detection Dataset (Entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with tanh for Credit Card Fraud Detection Dataset (Entire): Train\n",
      "Accuracy: 0.9989729905286905\n",
      "Sensitivity: 0.49746192893401014\n",
      "Specificity: 0.9998417248474404\n",
      "Precision: 0.8448275862068966\n",
      "False Discovery Rate: 0.15517241379310345\n",
      "F1 Score: 0.6261980830670927\n",
      "\n",
      "Logistic Regression with tanh for Credit Card Fraud Detection Dataset (Entire): Test\n",
      "Accuracy: 0.9990168711925703\n",
      "Sensitivity: 0.4897959183673469\n",
      "Specificity: 0.9998944832316269\n",
      "Precision: 0.8888888888888888\n",
      "False Discovery Rate: 0.1111111111111111\n",
      "F1 Score: 0.631578947368421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = train(\n",
    "    train_fraud_features, \n",
    "    train_fraud_target, \n",
    "    epochs=1000, \n",
    "    learning_rate=0.01, \n",
    "    report_loss=False, \n",
    "    is_weak_learning=True, \n",
    "    early_stopping_threshold=0\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(train_fraud_target, predict(train_fraud_features, w, is_weak_learning=True))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with tanh for Credit Card Fraud Detection Dataset (Entire): Train\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(test_fraud_target, predict(test_fraud_features, w, is_weak_learning=True))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with tanh for Credit Card Fraud Detection Dataset (Entire): Test\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit Card Fraud Detection Dataset (Subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with tanh for Credit Card Fraud Detection Dataset (Subsampled): Train\n",
      "Accuracy: 0.9951201659143589\n",
      "Sensitivity: 0.8045685279187818\n",
      "Specificity: 0.9998125\n",
      "Precision: 0.990625\n",
      "False Discovery Rate: 0.009375\n",
      "F1 Score: 0.8879551820728291\n",
      "\n",
      "Logistic Regression with tanh for Credit Card Fraud Detection Dataset (Subsampled): Test\n",
      "Accuracy: 0.9943875061005368\n",
      "Sensitivity: 0.7857142857142857\n",
      "Specificity: 0.9995\n",
      "Precision: 0.9746835443037974\n",
      "False Discovery Rate: 0.02531645569620253\n",
      "F1 Score: 0.8700564971751412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = train(\n",
    "    train_fraud_sub_features, \n",
    "    train_fraud_sub_target, \n",
    "    epochs=1000, \n",
    "    learning_rate=0.01, \n",
    "    report_loss=False, \n",
    "    is_weak_learning=True, \n",
    "    early_stopping_threshold=0\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(train_fraud_sub_target, predict(train_fraud_sub_features, w, is_weak_learning=True))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with tanh for Credit Card Fraud Detection Dataset (Subsampled): Train\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracy, \n",
    "    sensitivity, \n",
    "    specificity, \n",
    "    precision, \n",
    "    false_discovery_rate, \n",
    "    f1_score\n",
    ") = performance_evaluation(test_fraud_sub_target, predict(test_fraud_sub_features, w, is_weak_learning=True))\n",
    "\n",
    "print(\n",
    "    f'Logistic Regression with tanh for Credit Card Fraud Detection Dataset (Subsampled): Test\\n'\n",
    "    f'Accuracy: {accuracy}\\n'\n",
    "    f'Sensitivity: {sensitivity}\\n'\n",
    "    f'Specificity: {specificity}\\n'\n",
    "    f'Precision: {precision}\\n'\n",
    "    f'False Discovery Rate: {false_discovery_rate}\\n'\n",
    "    f'F1 Score: {f1_score}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Telco Customer Churn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost (5 -> 5 hypotheses) for Telco Customer Churn Dataset: Train\n",
      "Accuracy: 0.7823926162584309\n",
      "\n",
      "AdaBoost (5 -> 5 hypotheses) for Telco Customer Churn Dataset: Test\n",
      "Accuracy: 0.78708303761533\n",
      "\n",
      "AdaBoost (10 -> 10 hypotheses) for Telco Customer Churn Dataset: Train\n",
      "Accuracy: 0.7933972310969116\n",
      "\n",
      "AdaBoost (10 -> 10 hypotheses) for Telco Customer Churn Dataset: Test\n",
      "Accuracy: 0.7849538679914834\n",
      "\n",
      "AdaBoost (15 -> 15 hypotheses) for Telco Customer Churn Dataset: Train\n",
      "Accuracy: 0.7875399361022364\n",
      "\n",
      "AdaBoost (15 -> 15 hypotheses) for Telco Customer Churn Dataset: Test\n",
      "Accuracy: 0.7927608232789212\n",
      "\n",
      "AdaBoost (20 -> 20 hypotheses) for Telco Customer Churn Dataset: Train\n",
      "Accuracy: 0.7783102591409301\n",
      "\n",
      "AdaBoost (20 -> 20 hypotheses) for Telco Customer Churn Dataset: Test\n",
      "Accuracy: 0.7842441447835344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for K in range(5, 25, 5):\n",
    "    (\n",
    "        hypotheses, \n",
    "        hypothesis_weights\n",
    "    ) = adaptive_boosting(train_churn_features, train_churn_target, num_boosting_rounds=K, report_accuracy=False)\n",
    "    \n",
    "    (\n",
    "        accuracy, \n",
    "        sensitivity, \n",
    "        specificity, \n",
    "        precision, \n",
    "        false_discovery_rate, \n",
    "        f1_score\n",
    "    ) = performance_evaluation(\n",
    "        train_churn_target, \n",
    "        weighted_majority_predict(train_churn_features, hypotheses, hypothesis_weights)\n",
    "    )\n",
    "\n",
    "    print(f'AdaBoost ({K} -> {len(hypotheses)} hypotheses) for Telco Customer Churn Dataset: Train\\nAccuracy: {accuracy}\\n')\n",
    "    \n",
    "    (\n",
    "        accuracy, \n",
    "        sensitivity, \n",
    "        specificity, \n",
    "        precision, \n",
    "        false_discovery_rate, \n",
    "        f1_score\n",
    "    ) = performance_evaluation(\n",
    "        test_churn_target, \n",
    "        weighted_majority_predict(test_churn_features, hypotheses, hypothesis_weights)\n",
    "    )\n",
    "\n",
    "    print(f'AdaBoost ({K} -> {len(hypotheses)} hypotheses) for Telco Customer Churn Dataset: Test\\nAccuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adult Salary Scale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost (5 -> 5 hypotheses) for Adult Salary Scale Dataset: Train\n",
      "Accuracy: 0.8378735296827493\n",
      "\n",
      "AdaBoost (5 -> 5 hypotheses) for Adult Salary Scale Dataset: Test\n",
      "Accuracy: 0.836496529697193\n",
      "\n",
      "AdaBoost (10 -> 10 hypotheses) for Adult Salary Scale Dataset: Train\n",
      "Accuracy: 0.8385798961948343\n",
      "\n",
      "AdaBoost (10 -> 10 hypotheses) for Adult Salary Scale Dataset: Test\n",
      "Accuracy: 0.8374178490264725\n",
      "\n",
      "AdaBoost (15 -> 15 hypotheses) for Adult Salary Scale Dataset: Train\n",
      "Accuracy: 0.8400847639814502\n",
      "\n",
      "AdaBoost (15 -> 15 hypotheses) for Adult Salary Scale Dataset: Test\n",
      "Accuracy: 0.8368036361402862\n",
      "\n",
      "AdaBoost (20 -> 15 hypotheses) for Adult Salary Scale Dataset: Train\n",
      "Accuracy: 0.8372900095205921\n",
      "\n",
      "AdaBoost (20 -> 15 hypotheses) for Adult Salary Scale Dataset: Test\n",
      "Accuracy: 0.8370493212947607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for K in range(5, 25, 5):\n",
    "    (\n",
    "        hypotheses, \n",
    "        hypothesis_weights\n",
    "    ) = adaptive_boosting(train_salary_features, train_salary_target, num_boosting_rounds=K, report_accuracy=False)\n",
    "    \n",
    "    (\n",
    "        accuracy, \n",
    "        sensitivity, \n",
    "        specificity, \n",
    "        precision, \n",
    "        false_discovery_rate, \n",
    "        f1_score\n",
    "    ) = performance_evaluation(\n",
    "        train_salary_target, \n",
    "        weighted_majority_predict(train_salary_features, hypotheses, hypothesis_weights)\n",
    "    )\n",
    "\n",
    "    print(f'AdaBoost ({K} -> {len(hypotheses)} hypotheses) for Adult Salary Scale Dataset: Train\\nAccuracy: {accuracy}\\n')\n",
    "    \n",
    "    (\n",
    "        accuracy, \n",
    "        sensitivity, \n",
    "        specificity, \n",
    "        precision, \n",
    "        false_discovery_rate, \n",
    "        f1_score\n",
    "    ) = performance_evaluation(\n",
    "        test_salary_target, \n",
    "        weighted_majority_predict(test_salary_features, hypotheses, hypothesis_weights)\n",
    "    )\n",
    "\n",
    "    print(f'AdaBoost ({K} -> {len(hypotheses)} hypotheses) for Adult Salary Scale Dataset: Test\\nAccuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit Card Fraud Detection Dataset (Subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost (5 -> 5 hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Train\n",
      "Accuracy: 0.9951201659143589\n",
      "\n",
      "AdaBoost (5 -> 5 hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Test\n",
      "Accuracy: 0.9943875061005368\n",
      "\n",
      "AdaBoost (10 -> 9 hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Train\n",
      "Accuracy: 0.9951201659143589\n",
      "\n",
      "AdaBoost (10 -> 9 hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Test\n",
      "Accuracy: 0.9943875061005368\n",
      "\n",
      "AdaBoost (15 -> 11 hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Train\n",
      "Accuracy: 0.9951201659143589\n",
      "\n",
      "AdaBoost (15 -> 11 hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Test\n",
      "Accuracy: 0.9943875061005368\n",
      "\n",
      "AdaBoost (20 -> 13 hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Train\n",
      "Accuracy: 0.9951201659143589\n",
      "\n",
      "AdaBoost (20 -> 13 hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Test\n",
      "Accuracy: 0.9943875061005368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for K in range(5, 25, 5):\n",
    "    (\n",
    "        hypotheses, \n",
    "        hypothesis_weights\n",
    "    ) = adaptive_boosting(train_fraud_sub_features, train_fraud_sub_target, num_boosting_rounds=K, report_accuracy=False)\n",
    "    \n",
    "    (\n",
    "        accuracy, \n",
    "        sensitivity, \n",
    "        specificity, \n",
    "        precision, \n",
    "        false_discovery_rate, \n",
    "        f1_score\n",
    "    ) = performance_evaluation(\n",
    "        train_fraud_sub_target, \n",
    "        weighted_majority_predict(train_fraud_sub_features, hypotheses, hypothesis_weights)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'AdaBoost ({K} -> {len(hypotheses)} hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Train\\n'\n",
    "        f'Accuracy: {accuracy}\\n'\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        accuracy, \n",
    "        sensitivity, \n",
    "        specificity, \n",
    "        precision, \n",
    "        false_discovery_rate, \n",
    "        f1_score\n",
    "    ) = performance_evaluation(\n",
    "        test_fraud_sub_target, \n",
    "        weighted_majority_predict(test_fraud_sub_features, hypotheses, hypothesis_weights)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'AdaBoost ({K} -> {len(hypotheses)} hypotheses) for Credit Card Fraud Detection Dataset (Subsampled): Test\\n'\n",
    "        f'Accuracy: {accuracy}\\n'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
