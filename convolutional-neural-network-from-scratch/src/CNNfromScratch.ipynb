{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network from Scratch  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Component Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComponent(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, u):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def backward(self, del_v, lr):\n",
    "        pass\n",
    "    \n",
    "    def update_learnable_parameters(self, del_w, del_b, lr):\n",
    "        pass\n",
    "    \n",
    "    def save_learnable_parameters(self):\n",
    "        pass\n",
    "    \n",
    "    def set_learnable_parameters(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer(ModelComponent):\n",
    "    def __init__(self, num_filters, kernel_size, stride=1, padding=0):\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.weights_matrix = None\n",
    "        self.biases_vector = None\n",
    "        self.u_pad = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'Conv(filter={self.num_filters}, kernel={self.kernel_size}, stride={self.stride}, padding={self.padding})'\n",
    "    \n",
    "    def forward(self, u):\n",
    "        num_samples = u.shape[0]\n",
    "        input_dim = u.shape[1]\n",
    "        output_dim = math.floor((input_dim - self.kernel_size + 2 * self.padding) / self.stride) + 1\n",
    "        num_channels = u.shape[3]\n",
    "        \n",
    "        if self.weights is None:\n",
    "            # ref: https://cs231n.github.io/neural-networks-2/#init\n",
    "            # ref: https://stats.stackexchange.com/questions/198840/cnn-xavier-weight-initialization\n",
    "            self.weights = np.random.randn(self.num_filters, self.kernel_size, self.kernel_size, num_channels) * math.sqrt(2 / (self.kernel_size * self.kernel_size * num_channels))\n",
    "        if self.biases is None:\n",
    "            # ref: https://cs231n.github.io/neural-networks-2/#init\n",
    "            self.biases = np.zeros(self.num_filters)\n",
    "        \n",
    "        self.u_pad = np.pad(u, ((0,), (self.padding,), (self.padding,), (0,)), mode='constant')\n",
    "        v = np.zeros((num_samples, output_dim, output_dim, self.num_filters))\n",
    "        \n",
    "        for k in range(num_samples):\n",
    "            for l in range(self.num_filters):\n",
    "                for i in range(output_dim):\n",
    "                    for j in range(output_dim):\n",
    "                        v[k, i, j, l] = np.sum(self.u_pad[k, i * self.stride: i * self.stride + self.kernel_size, j * self.stride: j * self.stride + self.kernel_size, :] * self.weights[l]) + self.biases[l]\n",
    "        \n",
    "        return v\n",
    "    \n",
    "    def backward(self, del_v, lr):\n",
    "        num_samples = del_v.shape[0]\n",
    "        input_dim = del_v.shape[1]\n",
    "        input_dim_pad = (input_dim - 1) * self.stride + 1\n",
    "        output_dim = self.u_pad.shape[1] - 2 * self.padding\n",
    "        num_channels = self.u_pad.shape[3]\n",
    "        \n",
    "        del_b = np.sum(del_v, axis=(0, 1, 2)) / num_samples\n",
    "        del_v_sparse = np.zeros((num_samples, input_dim_pad, input_dim_pad, self.num_filters))\n",
    "        del_v_sparse[:, :: self.stride, :: self.stride, :] = del_v\n",
    "        weights_prime = np.rot90(np.transpose(self.weights, (3, 1, 2, 0)), 2, axes=(1, 2))\n",
    "        del_w = np.zeros((self.num_filters, self.kernel_size, self.kernel_size, num_channels))\n",
    "        \n",
    "        for l in range(self.num_filters):\n",
    "            for i in range(self.kernel_size):\n",
    "                for j in range(self.kernel_size):\n",
    "                    del_w[l, i, j, :] = np.mean(np.sum(self.u_pad[:, i: i + input_dim_pad, j: j + input_dim_pad, :] * np.reshape(del_v_sparse[:, :, :, l], del_v_sparse.shape[: 3] + (1,)), axis=(1, 2)), axis=0)\n",
    "        \n",
    "        del_u = np.zeros((num_samples, output_dim, output_dim, num_channels))\n",
    "        del_v_sparse_pad = np.pad(del_v_sparse, ((0,), (self.kernel_size - 1 - self.padding,), (self.kernel_size - 1 - self.padding,), (0,)), mode='constant')\n",
    "        \n",
    "        for k in range(num_samples):\n",
    "            for l in range(num_channels):\n",
    "                for i in range(output_dim):\n",
    "                    for j in range(output_dim):\n",
    "                        del_u[k, i, j, l] = np.sum(del_v_sparse_pad[k, i: i + self.kernel_size, j: j + self.kernel_size, :] * weights_prime[l])\n",
    "        \n",
    "        self.update_learnable_parameters(del_w, del_b, lr)\n",
    "        return del_u\n",
    "    \n",
    "    def update_learnable_parameters(self, del_w, del_b, lr):\n",
    "        self.weights = self.weights - lr * del_w\n",
    "        self.biases = self.biases - lr * del_b\n",
    "    \n",
    "    def save_learnable_parameters(self):\n",
    "        self.weights_matrix = np.copy(self.weights)\n",
    "        self.biases_vector = np.copy(self.biases)\n",
    "    \n",
    "    def set_learnable_parameters(self):\n",
    "        self.weights = self.weights if self.weights_matrix is None else np.copy(self.weights_matrix)\n",
    "        self.biases = self.biases if self.biases_vector is None else np.copy(self.biases_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(ModelComponent):\n",
    "    def __init__(self):\n",
    "        self.u = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'ReLU'\n",
    "    \n",
    "    def forward(self, u):\n",
    "        self.u = u\n",
    "        v = np.copy(u)\n",
    "        v[v < 0] = 0  # applying ReLU activation function\n",
    "        return v\n",
    "    \n",
    "    def backward(self, del_v, lr):\n",
    "        del_u = np.copy(self.u)\n",
    "        del_u[del_u > 0] = 1  # applying sign(x) function for x > 0\n",
    "        del_u[del_u < 0] = 0  # applying sign(x) function for x < 0\n",
    "        del_u = del_v * del_u\n",
    "        return del_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling Layer Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer(ModelComponent):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.u_shape = None\n",
    "        self.v_map = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'MaxPool(kernel={self.kernel_size}, stride={self.stride})'\n",
    "    \n",
    "    def forward(self, u):\n",
    "        self.u_shape = u.shape\n",
    "        \n",
    "        num_samples = u.shape[0]\n",
    "        input_dim = u.shape[1]\n",
    "        output_dim = math.floor((input_dim - self.kernel_size) / self.stride) + 1\n",
    "        num_channels = u.shape[3]\n",
    "        \n",
    "        v = np.zeros((num_samples, output_dim, output_dim, num_channels))\n",
    "        self.v_map = np.zeros((num_samples, output_dim, output_dim, num_channels)).astype(np.int32)\n",
    "        \n",
    "        for k in range(num_samples):\n",
    "            for l in range(num_channels):\n",
    "                for i in range(output_dim):\n",
    "                    for j in range(output_dim):\n",
    "                        v[k, i, j, l] = np.max(u[k, i * self.stride: i * self.stride + self.kernel_size, j * self.stride: j * self.stride + self.kernel_size, l])\n",
    "                        self.v_map[k, i, j, l] = np.argmax(u[k, i * self.stride: i * self.stride + self.kernel_size, j * self.stride: j * self.stride + self.kernel_size, l])\n",
    "        \n",
    "        return v\n",
    "    \n",
    "    def backward(self, del_v, lr):\n",
    "        del_u = np.zeros(self.u_shape)\n",
    "        \n",
    "        num_samples = del_v.shape[0]\n",
    "        input_dim = del_v.shape[1]\n",
    "        num_channels = del_v.shape[3]\n",
    "        \n",
    "        for k in range(num_samples):\n",
    "            for l in range(num_channels):\n",
    "                for i in range(input_dim):\n",
    "                    for j in range(input_dim):\n",
    "                        position = tuple(sum(pos) for pos in zip((self.v_map[k, i, j, l] // self.kernel_size, self.v_map[k, i, j, l] % self.kernel_size), (i * self.stride, j * self.stride)))\n",
    "                        del_u[(k,) + position + (l,)] = del_u[(k,) + position + (l,)] + del_v[k, i, j, l]\n",
    "        \n",
    "        return del_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening Layer Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatteningLayer(ModelComponent):\n",
    "    def __init__(self):\n",
    "        self.u_shape = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Flatten'\n",
    "    \n",
    "    def forward(self, u):\n",
    "        self.u_shape = u.shape\n",
    "        v = np.copy(u)\n",
    "        v = np.reshape(v, (v.shape[0], np.prod(v.shape[1:])))\n",
    "        v = np.transpose(v)\n",
    "        return v\n",
    "    \n",
    "    def backward(self, del_v, lr):\n",
    "        del_u = np.copy(del_v)\n",
    "        del_u = np.transpose(del_u)\n",
    "        del_u = np.reshape(del_u, self.u_shape)\n",
    "        return del_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(ModelComponent):\n",
    "    def __init__(self, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.weights_matrix = None\n",
    "        self.biases_vector = None\n",
    "        self.u = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'FullyConnected(output_dim={self.output_dim})'\n",
    "    \n",
    "    def forward(self, u):\n",
    "        self.u = u\n",
    "        \n",
    "        if self.weights is None:\n",
    "            # ref: https://cs231n.github.io/neural-networks-2/#init\n",
    "            self.weights = np.random.randn(self.output_dim, u.shape[0]) * math.sqrt(2 / u.shape[0])\n",
    "        if self.biases is None:\n",
    "            # ref: https://cs231n.github.io/neural-networks-2/#init\n",
    "            self.biases = np.zeros((self.output_dim, 1))\n",
    "        \n",
    "        v = self.weights @ u + self.biases\n",
    "        return v\n",
    "    \n",
    "    def backward(self, del_v, lr):\n",
    "        del_w = (del_v @ np.transpose(self.u)) / del_v.shape[1]\n",
    "        del_b = np.reshape(np.mean(del_v, axis=1), (del_v.shape[0], 1))\n",
    "        del_u = np.transpose(self.weights) @ del_v\n",
    "        self.update_learnable_parameters(del_w, del_b, lr)\n",
    "        return del_u\n",
    "    \n",
    "    def update_learnable_parameters(self, del_w, del_b, lr):\n",
    "        self.weights = self.weights - lr * del_w\n",
    "        self.biases = self.biases - lr * del_b\n",
    "    \n",
    "    def save_learnable_parameters(self):\n",
    "        self.weights_matrix = np.copy(self.weights)\n",
    "        self.biases_vector = np.copy(self.biases)\n",
    "    \n",
    "    def set_learnable_parameters(self):\n",
    "        self.weights = self.weights if self.weights_matrix is None else np.copy(self.weights_matrix)\n",
    "        self.biases = self.biases if self.biases_vector is None else np.copy(self.biases_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Layer Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer(ModelComponent):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Softmax'\n",
    "    \n",
    "    def forward(self, u):\n",
    "        v = np.exp(u)\n",
    "        v = v / np.sum(v, axis=0)\n",
    "        return v\n",
    "    \n",
    "    def backward(self, del_v, lr):\n",
    "        del_u = np.copy(del_v)\n",
    "        return del_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, model_path):\n",
    "        with open(model_path, 'r') as model_file:\n",
    "            model_specs = [model_spec.split() for model_spec in model_file.read().split('\\n') if model_spec != '']\n",
    "        \n",
    "        self.model_components = []\n",
    "        \n",
    "        for model_spec in model_specs:\n",
    "            if model_spec[0] == 'Conv':\n",
    "                self.model_components.append(ConvolutionLayer(num_filters=int(model_spec[1]), kernel_size=int(model_spec[2]), stride=int(model_spec[3]), padding=int(model_spec[4])))\n",
    "            elif model_spec[0] == 'ReLU':\n",
    "                self.model_components.append(ActivationLayer())\n",
    "            elif model_spec[0] == 'Pool':\n",
    "                self.model_components.append(MaxPoolingLayer(kernel_size=int(model_spec[1]), stride=int(model_spec[2])))\n",
    "            elif model_spec[0] == 'Flatten':\n",
    "                self.model_components.append(FlatteningLayer())\n",
    "            elif model_spec[0] == 'FC':\n",
    "                self.model_components.append(FullyConnectedLayer(output_dim=int(model_spec[1])))\n",
    "            elif model_spec[0] == 'Softmax':\n",
    "                self.model_components.append(SoftmaxLayer())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '\\n'.join(map(str, self.model_components))\n",
    "    \n",
    "    def train(self, u, y_true, lr):\n",
    "        for i in range(len(self.model_components)):\n",
    "            u = self.model_components[i].forward(u)\n",
    "        \n",
    "        del_v = u - y_true  # denoting y_predicted by u\n",
    "        \n",
    "        for i in range(len(self.model_components) - 1, -1, -1):\n",
    "            del_v = self.model_components[i].backward(del_v, lr)\n",
    "    \n",
    "    def predict(self, u):\n",
    "        for i in range(len(self.model_components)):\n",
    "            u = self.model_components[i].forward(u)\n",
    "        \n",
    "        return u  # denoting y_predicted by u\n",
    "    \n",
    "    def save_model(self):\n",
    "        for i in range(len(self.model_components)):\n",
    "            self.model_components[i].save_learnable_parameters()\n",
    "    \n",
    "    def set_model(self):\n",
    "        for i in range(len(self.model_components)):\n",
    "            self.model_components[i].set_learnable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset Loader Definition  \n",
    "\n",
    "**Ref:** https://keras.io/api/datasets/mnist/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset():\n",
    "    mnist_dataset = datasets.mnist.load_data()\n",
    "    (x_train, y_train), (x_evaluation, y_evaluation) = mnist_dataset\n",
    "\n",
    "    x_train = np.reshape(x_train, (*x_train.shape, 1)).astype(np.float32)\n",
    "    y_train = np.reshape(y_train, (*y_train.shape, 1))\n",
    "    x_evaluation = np.reshape(x_evaluation, (*x_evaluation.shape, 1)).astype(np.float32)\n",
    "    y_evaluation = np.reshape(y_evaluation, (*y_evaluation.shape, 1))\n",
    "\n",
    "    x_train = x_train / 255\n",
    "    x_evaluation = x_evaluation / 255\n",
    "\n",
    "    x_validation, x_test, y_validation, y_test = train_test_split(x_evaluation, y_evaluation, test_size=0.5, random_state=0)\n",
    "    return x_train, y_train, x_validation, y_validation, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Dataset Loader Definition  \n",
    "\n",
    "**Ref:** https://keras.io/api/datasets/cifar10/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar_10_dataset():\n",
    "    cifar_10_dataset = datasets.cifar10.load_data()\n",
    "    (x_train, y_train), (x_evaluation, y_evaluation) = cifar_10_dataset\n",
    "\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    x_evaluation = x_evaluation.astype(np.float32)\n",
    "\n",
    "    x_train = x_train / 255\n",
    "    x_evaluation = x_evaluation / 255\n",
    "\n",
    "    x_validation, x_test, y_validation, y_test = train_test_split(x_evaluation, y_evaluation, test_size=0.5, random_state=0)\n",
    "    return x_train, y_train, x_validation, y_validation, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Subsampler Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_dataset(num_classes, num_samples_per_class, x, y):\n",
    "    indices = []\n",
    "    \n",
    "    for n_class in range(num_classes):\n",
    "        indices.append(np.where(y == n_class)[0][: num_samples_per_class])\n",
    "    \n",
    "    indices = np.concatenate(indices, axis=0)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    x = np.take(x, indices, axis=0)\n",
    "    y = np.take(y, indices, axis=0)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Scorers Definition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cross_entropy_loss(y_true, y_predicted):\n",
    "    return np.sum(-1 * np.sum(y_true * np.log(y_predicted), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_scores(num_classes, y_true, y_predicted):\n",
    "    true_positives = np.zeros(num_classes)\n",
    "    false_positives = np.zeros(num_classes)\n",
    "    false_negatives = np.zeros(num_classes)\n",
    "    \n",
    "    for i in range(y_true.shape[0]):\n",
    "        if y_true[i, 0] == y_predicted[i, 0]:\n",
    "            true_positives[y_true[i, 0]] = true_positives[y_true[i, 0]] + 1\n",
    "        else:\n",
    "            false_positives[y_predicted[i, 0]] = false_positives[y_predicted[i, 0]] + 1\n",
    "            false_negatives[y_true[i, 0]] = false_negatives[y_true[i, 0]] + 1\n",
    "    \n",
    "    # ref: https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f\n",
    "    accuracy = np.sum(true_positives) / (np.sum(true_positives) + 0.5 * (np.sum(false_positives) + np.sum(false_negatives)))  # micro/global average f1 score\n",
    "    f1_score = np.mean(true_positives / (true_positives + 0.5 * (false_positives + false_negatives)))  # macro average f1 score\n",
    "    return accuracy, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Initialization  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model_path='inputdir/model.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Configuration  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mnist = True  # use_mnist -> True: Use MNIST; False: Use CIFAR-10;\n",
    "num_classes = 10\n",
    "num_samples_per_class = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 32\n",
    "num_epochs = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training, Validation and Testing  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_mnist_dataset() if use_mnist else load_cifar_10_dataset()\n",
    "x_train, y_train = subsample_dataset(num_classes, num_samples_per_class, x_train, y_train)\n",
    "x_validation, y_validation = subsample_dataset(num_classes, num_samples_per_class // 10, x_validation, y_validation)\n",
    "x_test, y_test = subsample_dataset(num_classes, num_samples_per_class // 10, x_test, y_test)\n",
    "\n",
    "num_batches = math.ceil(y_train.shape[0] / num_samples)\n",
    "min_f1_score = math.inf\n",
    "validation_stats = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(num_batches):\n",
    "        print(f'(Training) Epoch: {epoch + 1} -> {batch + 1}/{num_batches} Batches Trained.', end='\\r')\n",
    "        n_samples = y_train.shape[0] - batch * num_samples if (batch + 1) * num_samples > y_train.shape[0] else num_samples\n",
    "        y_true = np.zeros((num_classes, n_samples))\n",
    "        \n",
    "        for i in range(y_true.shape[1]):\n",
    "            y_true[y_train[batch * num_samples + i, 0], i] = 1  # generating one-hot encoding of y_train\n",
    "        \n",
    "        model.train(x_train[batch * num_samples: batch * num_samples + n_samples], y_true, lr)\n",
    "    print()\n",
    "    \n",
    "    y_true = np.zeros((num_classes, y_validation.shape[0]))\n",
    "    y_predicted = model.predict(x_validation)\n",
    "    \n",
    "    for i in range(y_true.shape[1]):\n",
    "        y_true[y_validation[i, 0], i] = 1  # generating one-hot encoding of y_validation\n",
    "    \n",
    "    cross_entropy_loss = calculate_cross_entropy_loss(y_true, y_predicted)\n",
    "    accuracy, f1_score = calculate_f1_scores(num_classes, y_validation, np.reshape(np.argmax(y_predicted, axis=0), y_validation.shape))\n",
    "    \n",
    "    if f1_score < min_f1_score:\n",
    "        min_f1_score = f1_score\n",
    "        model.save_model()\n",
    "    \n",
    "    validation_stats.append([epoch + 1, cross_entropy_loss, accuracy, f1_score])\n",
    "    print(f'\\n(Validation) Epoch: {epoch + 1} -> CE Loss: {cross_entropy_loss:.4f}\\tAccuracy: {accuracy:.4f}\\tF1 Score: {f1_score:.4f}\\n')\n",
    "\n",
    "if not os.path.exists('outputdir/'):\n",
    "    os.makedirs('outputdir/')\n",
    "\n",
    "with open('outputdir/validation_stats.csv', 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file) \n",
    "    csv_writer.writerow(['Epoch', 'CE Loss', 'Accuracy', 'F1 Score']) \n",
    "    csv_writer.writerows(validation_stats)\n",
    "\n",
    "model.set_model()\n",
    "\n",
    "y_true = np.zeros((num_classes, y_test.shape[0]))\n",
    "y_predicted = model.predict(x_test)\n",
    "\n",
    "for i in range(y_true.shape[1]):\n",
    "    y_true[y_test[i, 0], i] = 1  # generating one-hot encoding of y_test\n",
    "\n",
    "cross_entropy_loss = calculate_cross_entropy_loss(y_true, y_predicted)\n",
    "accuracy, f1_score = calculate_f1_scores(num_classes, y_test, np.reshape(np.argmax(y_predicted, axis=0), y_test.shape))\n",
    "\n",
    "test_stats = [[cross_entropy_loss, accuracy, f1_score]]\n",
    "print(f'(Testing) -> CE Loss: {cross_entropy_loss:.4f}\\tAccuracy: {accuracy:.4f}\\tF1 Score: {f1_score:.4f}')\n",
    "\n",
    "with open('outputdir/test_stats.csv', 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file) \n",
    "    csv_writer.writerow(['CE Loss', 'Accuracy', 'F1 Score']) \n",
    "    csv_writer.writerows(test_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
